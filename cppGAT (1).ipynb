{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URVW40hKq-Cc"
      },
      "source": [
        "# Graph Attention Networks for Prediction of Penetration and Toxicity of Cell Penetrating Peptides\n",
        "\n",
        "This notebook implements a Graph Attention Network (GAT) approach to predict:\n",
        "1. Cell penetration capability (CPP)\n",
        "2. Toxicity\n",
        "\n",
        "of peptide molecules using their SMILES representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkoncg56q-Cd"
      },
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTyFz-Idq-Cd",
        "outputId": "06416910-48bf-4ac6-bc3c-0f315bc0637a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.7.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -U torch torchvision\n",
        "!pip install -U torch-geometric\n",
        "!pip install -U torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.7.0+${CUDA}.html\n",
        "!pip install -U dkit matplotlib seaborn pandas scikit-learn networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeewOIFFq-Ce",
        "outputId": "3ffd6a66-8c22-464e-92f6-a073d3008c49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_scatter/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv, global_mean_pool, global_add_pool\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "\n",
        "from rdkit import Chem\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tMSwnfZq-Ce"
      },
      "source": [
        "## 2. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ1VKygWq-Ce"
      },
      "outputs": [],
      "source": [
        "# Load the datasets\n",
        "train_data = pd.read_csv('train_peptides.csv')\n",
        "val_data = pd.read_csv('val_peptides.csv')\n",
        "test_data = pd.read_csv('test_peptides.csv')\n",
        "\n",
        "# Display basic information\n",
        "print(f\"Training set: {train_data.shape[0]} samples, {train_data.shape[1]} features\")\n",
        "print(f\"Validation set: {val_data.shape[0]} samples, {val_data.shape[1]} features\")\n",
        "print(f\"Test set: {test_data.shape[0]} samples, {test_data.shape[1]} features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcHDYZecq-Ce"
      },
      "outputs": [],
      "source": [
        "# Check target variable distributions\n",
        "print(\"Distribution of CPP status (training data):\")\n",
        "print(train_data['CPP?'].value_counts())\n",
        "print(f\"Percentage of CPPs: {train_data['CPP?'].mean() * 100:.1f}%\")\n",
        "\n",
        "print(\"\\nDistribution of toxicity status (training data):\")\n",
        "print(train_data['toxic?'].value_counts())\n",
        "print(f\"Percentage of toxic peptides: {train_data['toxic?'].mean() * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfgLB2KTq-Ce"
      },
      "source": [
        "## 3. Data Preprocessing: Converting SMILES to Molecular Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6-zucf9q-Ce"
      },
      "outputs": [],
      "source": [
        "def smiles_to_graph(smiles_string):\n",
        "    \"\"\"Convert a SMILES string to a PyTorch Geometric graph.\"\"\"\n",
        "    # Handle invalid SMILES\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles_string)\n",
        "        if mol is None:\n",
        "            return None\n",
        "        # Add hydrogens to get more complete molecular representation\n",
        "        mol = Chem.AddHs(mol)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "    # Extract atom features\n",
        "    node_features = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        # Atom features - using common physicochemical properties relevant for peptides\n",
        "        features = [\n",
        "            # One-hot encoding of atom type (C, N, O, S, Other)\n",
        "            atom.GetSymbol() == 'C',\n",
        "            atom.GetSymbol() == 'N',\n",
        "            atom.GetSymbol() == 'O',\n",
        "            atom.GetSymbol() == 'S',\n",
        "            atom.GetSymbol() not in ['C', 'N', 'O', 'S'],\n",
        "\n",
        "            # Atom properties\n",
        "            atom.GetAtomicNum(),          # Atomic number\n",
        "            atom.GetFormalCharge(),       # Formal charge\n",
        "            atom.GetTotalDegree(),        # Total degree\n",
        "            atom.GetTotalNumHs(),         # Total number of hydrogens\n",
        "            atom.GetIsAromatic(),         # Is aromatic\n",
        "            atom.GetNumRadicalElectrons(), # Number of radical electrons\n",
        "            atom.IsInRing(),               # Is in ring\n",
        "            atom.GetHybridization() == Chem.rdchem.HybridizationType.SP,  # SP hybridization\n",
        "            atom.GetHybridization() == Chem.rdchem.HybridizationType.SP2, # SP2 hybridization\n",
        "            atom.GetHybridization() == Chem.rdchem.HybridizationType.SP3, # SP3 hybridization\n",
        "        ]\n",
        "        node_features.append(features)\n",
        "\n",
        "    # Convert node features to tensor\n",
        "    x = torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "    # Extract edge indices and features\n",
        "    edge_indices = []\n",
        "    edge_features = []\n",
        "\n",
        "    for bond in mol.GetBonds():\n",
        "        # Add edges in both directions (for undirected graph)\n",
        "        i = bond.GetBeginAtomIdx()\n",
        "        j = bond.GetEndAtomIdx()\n",
        "\n",
        "        edge_indices.append([i, j])\n",
        "        edge_indices.append([j, i])  # Add reverse edge for undirected graph\n",
        "\n",
        "        # Bond features\n",
        "        bond_type = bond.GetBondType()\n",
        "        features = [\n",
        "            bond_type == Chem.rdchem.BondType.SINGLE,\n",
        "            bond_type == Chem.rdchem.BondType.DOUBLE,\n",
        "            bond_type == Chem.rdchem.BondType.TRIPLE,\n",
        "            bond_type == Chem.rdchem.BondType.AROMATIC,\n",
        "            bond.IsInRing(),\n",
        "            bond.GetIsConjugated(),\n",
        "        ]\n",
        "\n",
        "        # Add features for both directions\n",
        "        edge_features.append(features)\n",
        "        edge_features.append(features)  # Same features for reverse edge\n",
        "\n",
        "    if len(edge_indices) == 0:  # Handle molecules with no bonds\n",
        "        edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
        "        edge_attr = torch.zeros((0, 6), dtype=torch.float)  # 6 is the number of edge features\n",
        "    else:\n",
        "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
        "        edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN9VTdXPq-Cf"
      },
      "outputs": [],
      "source": [
        "# Create a function to prepare the datasets\n",
        "def prepare_dataset(df):\n",
        "    \"\"\"Convert a dataframe of peptides to a list of graph data objects.\"\"\"\n",
        "    graphs = []\n",
        "    processed_count = 0\n",
        "    invalid_count = 0\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        smiles = row['smiles']\n",
        "        graph_data = smiles_to_graph(smiles)\n",
        "\n",
        "        if graph_data is None:\n",
        "            invalid_count += 1\n",
        "            continue\n",
        "\n",
        "        # Add targets: CPP and toxicity\n",
        "        graph_data.cpp = torch.tensor([row['CPP?']], dtype=torch.float)\n",
        "        graph_data.toxic = torch.tensor([row['toxic?']], dtype=torch.float)\n",
        "\n",
        "        # Add some additional features at the graph level\n",
        "        graph_data.peptide_length = torch.tensor([row['len']], dtype=torch.float)\n",
        "        graph_data.is_cyclic = torch.tensor([float(row['is_cyclic'])], dtype=torch.float)\n",
        "\n",
        "        # Add additional features if they exist\n",
        "        additional_features = []\n",
        "        for feature in ['average_wt', 'SVG']:\n",
        "            if feature in row:\n",
        "                additional_features.append(float(row[feature]))\n",
        "\n",
        "        # Add extra features in val and test sets if available\n",
        "        for feature in ['Hydrophobicity', 'Hydropathicity', 'Hydrophilicity', 'Charge']:\n",
        "            if feature in row:\n",
        "                additional_features.append(float(row[feature]))\n",
        "\n",
        "        if additional_features:\n",
        "            graph_data.additional_features = torch.tensor([additional_features], dtype=torch.float)\n",
        "\n",
        "        graphs.append(graph_data)\n",
        "        processed_count += 1\n",
        "\n",
        "        # Progress report for large datasets\n",
        "        if processed_count % 200 == 0:\n",
        "            print(f\"Processed {processed_count} peptides...\")\n",
        "\n",
        "    print(f\"Successfully processed {processed_count} peptides. Invalid SMILES: {invalid_count}\")\n",
        "    return graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEpWtdOAq-Cf"
      },
      "outputs": [],
      "source": [
        "# Process the datasets\n",
        "print(\"Processing training dataset...\")\n",
        "train_graphs = prepare_dataset(train_data)\n",
        "\n",
        "print(\"\\nProcessing validation dataset...\")\n",
        "val_graphs = prepare_dataset(val_data)\n",
        "\n",
        "print(\"\\nProcessing test dataset...\")\n",
        "test_graphs = prepare_dataset(test_data)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_graphs, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_graphs, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YttwanO1q-Cf"
      },
      "outputs": [],
      "source": [
        "# Check the first batch to understand the data structure\n",
        "for batch in train_loader:\n",
        "    print(\"Batch information:\")\n",
        "    print(f\"- Number of graphs in batch: {batch.num_graphs}\")\n",
        "    print(f\"- Node feature shape: {batch.x.shape}\")\n",
        "    print(f\"- Edge index shape: {batch.edge_index.shape}\")\n",
        "    print(f\"- CPP targets shape: {batch.cpp.shape}\")\n",
        "    print(f\"- Toxicity targets shape: {batch.toxic.shape}\")\n",
        "    if hasattr(batch, 'additional_features'):\n",
        "        print(f\"- Additional features shape: {batch.additional_features.shape}\")\n",
        "    break  # Just check one batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkyO23B9q-Cf"
      },
      "source": [
        "## 4. Define the Graph Attention Network Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxKVTxGRq-Cf"
      },
      "outputs": [],
      "source": [
        "class GATForPeptidesModel(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_layers=3, heads=4, dropout=0.2):\n",
        "        super(GATForPeptidesModel, self).__init__()\n",
        "\n",
        "        # Graph Attention layers\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "\n",
        "        # First GAT layer\n",
        "        self.gat_layers.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n",
        "\n",
        "        # Middle GAT layers\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.gat_layers.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout))\n",
        "\n",
        "        # Last GAT layer\n",
        "        self.gat_layers.append(GATConv(hidden_channels * heads, hidden_channels, heads=1, dropout=dropout))\n",
        "\n",
        "        # Output layers for the two prediction tasks\n",
        "        # We'll also consider additional molecule-level features\n",
        "        self.global_pool = global_mean_pool\n",
        "\n",
        "        # For CPP prediction\n",
        "        self.cpp_predictor = nn.Sequential(\n",
        "            nn.Linear(hidden_channels + 4, hidden_channels),  # +4 for additional features\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_channels, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # For toxicity prediction\n",
        "        self.tox_predictor = nn.Sequential(\n",
        "            nn.Linear(hidden_channels + 4, hidden_channels),  # +4 for additional features\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_channels, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Attention for interpretability - separate attention weights for CPP and toxicity\n",
        "        self.cpp_attention = nn.Sequential(\n",
        "            nn.Linear(hidden_channels, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.tox_attention = nn.Sequential(\n",
        "            nn.Linear(hidden_channels, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, batch, additional_features=None):\n",
        "        # Process through GAT layers\n",
        "        for gat_layer in self.gat_layers:\n",
        "            x = gat_layer(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=0.2, training=self.training)\n",
        "\n",
        "        # Global pooling to get graph-level representation\n",
        "        pooled = self.global_pool(x, batch)\n",
        "\n",
        "        # Create attention weights for interpretability\n",
        "        cpp_attn_weights = self.cpp_attention(x).squeeze()\n",
        "        tox_attn_weights = self.tox_attention(x).squeeze()\n",
        "\n",
        "        # Weighted pooling for interpretable predictions\n",
        "        cpp_pooled = global_add_pool(x * cpp_attn_weights.unsqueeze(-1), batch)\n",
        "        tox_pooled = global_add_pool(x * tox_attn_weights.unsqueeze(-1), batch)\n",
        "\n",
        "        # If we have additional features, concatenate them\n",
        "        if additional_features is not None:\n",
        "            cpp_pooled = torch.cat([cpp_pooled, additional_features], dim=1)\n",
        "            tox_pooled = torch.cat([tox_pooled, additional_features], dim=1)\n",
        "        else:\n",
        "            # Add zeros as placeholder for additional features (to keep dimensions consistent)\n",
        "            batch_size = cpp_pooled.size(0)\n",
        "            dummy_features = torch.zeros(batch_size, 4, device=cpp_pooled.device)\n",
        "            cpp_pooled = torch.cat([cpp_pooled, dummy_features], dim=1)\n",
        "            tox_pooled = torch.cat([tox_pooled, dummy_features], dim=1)\n",
        "\n",
        "        # Make predictions\n",
        "        cpp_pred = self.cpp_predictor(cpp_pooled)\n",
        "        tox_pred = self.tox_predictor(tox_pooled)\n",
        "\n",
        "        return cpp_pred, tox_pred, cpp_attn_weights, tox_attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pIkPm4Tq-Cg"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "# First, determine the input feature dimension from the data\n",
        "for batch in train_loader:\n",
        "    input_dim = batch.x.shape[1]\n",
        "    break\n",
        "\n",
        "# Define the device for training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create the model\n",
        "model = GATForPeptidesModel(\n",
        "    in_channels=input_dim,\n",
        "    hidden_channels=64,\n",
        "    num_layers=3,\n",
        "    heads=4,\n",
        "    dropout=0.2\n",
        ").to(device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "# Define the loss functions and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U15aWDBq-Cg"
      },
      "source": [
        "## 5. Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnU7dOwnq-Cg"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    cpp_predictions = []\n",
        "    cpp_targets = []\n",
        "    tox_predictions = []\n",
        "    tox_targets = []\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get additional features if they exist\n",
        "        additional_features = None\n",
        "        if hasattr(batch, 'additional_features'):\n",
        "            additional_features = batch.additional_features\n",
        "        else:\n",
        "            # Create a tensor of zeros as placeholder for additional features\n",
        "            additional_features = torch.zeros(batch.num_graphs, 4, device=device)\n",
        "\n",
        "        # Forward pass\n",
        "        cpp_pred, tox_pred, _, _ = model(batch.x, batch.edge_index, batch.batch, additional_features)\n",
        "\n",
        "        # Calculate loss\n",
        "        cpp_loss = criterion(cpp_pred, batch.cpp)\n",
        "        tox_loss = criterion(tox_pred, batch.toxic)\n",
        "        loss = cpp_loss + tox_loss  # Combined loss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * batch.num_graphs\n",
        "\n",
        "        # Store predictions and targets for metrics calculation\n",
        "        cpp_predictions.extend(cpp_pred.detach().cpu().numpy())\n",
        "        cpp_targets.extend(batch.cpp.detach().cpu().numpy())\n",
        "        tox_predictions.extend(tox_pred.detach().cpu().numpy())\n",
        "        tox_targets.extend(batch.toxic.detach().cpu().numpy())\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    cpp_predictions = np.array(cpp_predictions)\n",
        "    cpp_targets = np.array(cpp_targets)\n",
        "    tox_predictions = np.array(tox_predictions)\n",
        "    tox_targets = np.array(tox_targets)\n",
        "\n",
        "    # Calculate metrics\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    cpp_accuracy = accuracy_score(cpp_targets > 0.5, cpp_predictions > 0.5)\n",
        "    cpp_auc = roc_auc_score(cpp_targets, cpp_predictions)\n",
        "    tox_accuracy = accuracy_score(tox_targets > 0.5, tox_predictions > 0.5)\n",
        "    tox_auc = roc_auc_score(tox_targets, tox_predictions)\n",
        "\n",
        "    return avg_loss, cpp_accuracy, cpp_auc, tox_accuracy, tox_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLjEDnccq-Cg"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader, device, threshold=0.5):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    cpp_predictions = []\n",
        "    cpp_targets = []\n",
        "    tox_predictions = []\n",
        "    tox_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Get additional features if they exist\n",
        "            additional_features = None\n",
        "            if hasattr(batch, 'additional_features'):\n",
        "                additional_features = batch.additional_features\n",
        "            else:\n",
        "                # Create a tensor of zeros as placeholder for additional features\n",
        "                additional_features = torch.zeros(batch.num_graphs, 4, device=device)\n",
        "\n",
        "            # Forward pass\n",
        "            cpp_pred, tox_pred, _, _ = model(batch.x, batch.edge_index, batch.batch, additional_features)\n",
        "\n",
        "            # Calculate loss\n",
        "            cpp_loss = criterion(cpp_pred, batch.cpp)\n",
        "            tox_loss = criterion(tox_pred, batch.toxic)\n",
        "            loss = cpp_loss + tox_loss\n",
        "\n",
        "            total_loss += loss.item() * batch.num_graphs\n",
        "\n",
        "            # Store predictions and targets\n",
        "            cpp_predictions.extend(cpp_pred.cpu().numpy())\n",
        "            cpp_targets.extend(batch.cpp.cpu().numpy())\n",
        "            tox_predictions.extend(tox_pred.cpu().numpy())\n",
        "            tox_targets.extend(batch.toxic.cpu().numpy())\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    cpp_predictions = np.array(cpp_predictions)\n",
        "    cpp_targets = np.array(cpp_targets)\n",
        "    tox_predictions = np.array(tox_predictions)\n",
        "    tox_targets = np.array(tox_targets)\n",
        "\n",
        "    # Calculate metrics\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "\n",
        "    # Classification metrics\n",
        "    cpp_accuracy = accuracy_score(cpp_targets > 0.5, cpp_predictions > threshold)\n",
        "    cpp_precision, cpp_recall, cpp_f1, _ = precision_recall_fscore_support(\n",
        "        cpp_targets > 0.5, cpp_predictions > threshold, average='binary')\n",
        "    cpp_auc = roc_auc_score(cpp_targets, cpp_predictions)\n",
        "\n",
        "    tox_accuracy = accuracy_score(tox_targets > 0.5, tox_predictions > threshold)\n",
        "    tox_precision, tox_recall, tox_f1, _ = precision_recall_fscore_support(\n",
        "        tox_targets > 0.5, tox_predictions > threshold, average='binary')\n",
        "    tox_auc = roc_auc_score(tox_targets, tox_predictions)\n",
        "\n",
        "    # Confusion matrices\n",
        "    cpp_cm = confusion_matrix(cpp_targets > 0.5, cpp_predictions > threshold)\n",
        "    tox_cm = confusion_matrix(tox_targets > 0.5, tox_predictions > threshold)\n",
        "\n",
        "    return {\n",
        "        'loss': avg_loss,\n",
        "        'cpp_accuracy': cpp_accuracy,\n",
        "        'cpp_precision': cpp_precision,\n",
        "        'cpp_recall': cpp_recall,\n",
        "        'cpp_f1': cpp_f1,\n",
        "        'cpp_auc': cpp_auc,\n",
        "        'cpp_cm': cpp_cm,\n",
        "        'tox_accuracy': tox_accuracy,\n",
        "        'tox_precision': tox_precision,\n",
        "        'tox_recall': tox_recall,\n",
        "        'tox_f1': tox_f1,\n",
        "        'tox_auc': tox_auc,\n",
        "        'tox_cm': tox_cm,\n",
        "        'cpp_predictions': cpp_predictions,\n",
        "        'cpp_targets': cpp_targets,\n",
        "        'tox_predictions': tox_predictions,\n",
        "        'tox_targets': tox_targets\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE2mukzfq-Cg"
      },
      "source": [
        "## 6. Model Training and Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAS1ZH-uq-Cg"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "num_epochs = 30\n",
        "best_val_auc = 0\n",
        "best_model_state = None\n",
        "patience = 10\n",
        "counter = 0\n",
        "\n",
        "# Initialize tracking variables\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_cpp_accs = []\n",
        "val_cpp_accs = []\n",
        "train_tox_accs = []\n",
        "val_tox_accs = []\n",
        "train_cpp_aucs = []\n",
        "val_cpp_aucs = []\n",
        "train_tox_aucs = []\n",
        "val_tox_aucs = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    train_loss, train_cpp_acc, train_cpp_auc, train_tox_acc, train_tox_auc = train_epoch(\n",
        "        model, train_loader, optimizer, device)\n",
        "\n",
        "    # Validation\n",
        "    val_metrics = evaluate(model, val_loader, device)\n",
        "\n",
        "    # Update learning rate scheduler\n",
        "    val_loss = val_metrics['loss']\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Track metrics\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_cpp_accs.append(train_cpp_acc)\n",
        "    val_cpp_accs.append(val_metrics['cpp_accuracy'])\n",
        "    train_tox_accs.append(train_tox_acc)\n",
        "    val_tox_accs.append(val_metrics['tox_accuracy'])\n",
        "    train_cpp_aucs.append(train_cpp_auc)\n",
        "    val_cpp_aucs.append(val_metrics['cpp_auc'])\n",
        "    train_tox_aucs.append(train_tox_auc)\n",
        "    val_tox_aucs.append(val_metrics['tox_auc'])\n",
        "\n",
        "    # Log progress\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"  CPP - Train Acc: {train_cpp_acc:.4f}, Val Acc: {val_metrics['cpp_accuracy']:.4f}, Val AUC: {val_metrics['cpp_auc']:.4f}\")\n",
        "    print(f\"  Tox - Train Acc: {train_tox_acc:.4f}, Val Acc: {val_metrics['tox_accuracy']:.4f}, Val AUC: {val_metrics['tox_auc']:.4f}\")\n",
        "\n",
        "    # Save the best model based on validation AUC (average of both tasks)\n",
        "    val_auc_avg = (val_metrics['cpp_auc'] + val_metrics['tox_auc']) / 2\n",
        "    if val_auc_avg > best_val_auc:\n",
        "        best_val_auc = val_auc_avg\n",
        "        best_model_state = model.state_dict().copy()\n",
        "        counter = 0\n",
        "        print(f\"  New best model saved! Average AUC: {val_auc_avg:.4f}\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping after {epoch+1} epochs (no improvement for {patience} epochs)\")\n",
        "            break\n",
        "\n",
        "    print(\"\")\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(best_model_state)\n",
        "print(f\"Training completed! Best validation AUC: {best_val_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2Rbo1F6q-Cg"
      },
      "source": [
        "## 7. Training Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0Sqy1IIq-Ch"
      },
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot losses\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot CPP accuracy\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(train_cpp_accs, label='Train CPP Accuracy')\n",
        "plt.plot(val_cpp_accs, label='Validation CPP Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('CPP Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot toxicity accuracy\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(train_tox_accs, label='Train Toxicity Accuracy')\n",
        "plt.plot(val_tox_accs, label='Validation Toxicity Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Toxicity Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot AUC\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(train_cpp_aucs, label='Train CPP AUC')\n",
        "plt.plot(val_cpp_aucs, label='Validation CPP AUC')\n",
        "plt.plot(train_tox_aucs, label='Train Toxicity AUC')\n",
        "plt.plot(val_tox_aucs, label='Validation Toxicity AUC')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUC')\n",
        "plt.title('ROC AUC Scores')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NzOGWA8q-Ch"
      },
      "source": [
        "## 8. Model Evaluation on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFRsjNXnq-Ch"
      },
      "outputs": [],
      "source": [
        "# Evaluate on the test set\n",
        "test_metrics = evaluate(model, test_loader, device)\n",
        "\n",
        "print(\"\\nTest Set Performance:\")\n",
        "print(f\"  CPP - Accuracy: {test_metrics['cpp_accuracy']:.4f}\")\n",
        "print(f\"  CPP - Precision: {test_metrics['cpp_precision']:.4f}\")\n",
        "print(f\"  CPP - Recall: {test_metrics['cpp_recall']:.4f}\")\n",
        "print(f\"  CPP - F1 Score: {test_metrics['cpp_f1']:.4f}\")\n",
        "print(f\"  CPP - AUC: {test_metrics['cpp_auc']:.4f}\")\n",
        "print(\"\\n\")\n",
        "print(f\"  Toxicity - Accuracy: {test_metrics['tox_accuracy']:.4f}\")\n",
        "print(f\"  Toxicity - Precision: {test_metrics['tox_precision']:.4f}\")\n",
        "print(f\"  Toxicity - Recall: {test_metrics['tox_recall']:.4f}\")\n",
        "print(f\"  Toxicity - F1 Score: {test_metrics['tox_f1']:.4f}\")\n",
        "print(f\"  Toxicity - AUC: {test_metrics['tox_auc']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OH1Z4gIIq-Ch"
      },
      "outputs": [],
      "source": [
        "# Confusion matrices\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(test_metrics['cpp_cm'], annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Non-CPP', 'CPP'], yticklabels=['Non-CPP', 'CPP'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('CPP Prediction Confusion Matrix')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(test_metrics['tox_cm'], annot=True, fmt='d', cmap='Reds',\n",
        "            xticklabels=['Non-Toxic', 'Toxic'], yticklabels=['Non-Toxic', 'Toxic'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Toxicity Prediction Confusion Matrix')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_yHt6IRq-Ch"
      },
      "outputs": [],
      "source": [
        "# ROC curves\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Plot CPP ROC curve\n",
        "plt.subplot(1, 2, 1)\n",
        "fpr, tpr, _ = roc_curve(test_metrics['cpp_targets'], test_metrics['cpp_predictions'])\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {test_metrics['cpp_auc']:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - CPP Prediction')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot Toxicity ROC curve\n",
        "plt.subplot(1, 2, 2)\n",
        "fpr, tpr, _ = roc_curve(test_metrics['tox_targets'], test_metrics['tox_predictions'])\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {test_metrics['tox_auc']:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Toxicity Prediction')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdqXyITRq-Ch"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), 'gat_peptide_model.pt')\n",
        "print(\"Model saved to 'gat_peptide_model.pt'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuC7hwbAq-Ch"
      },
      "source": [
        "## 9. Conclusion\n",
        "\n",
        "In this notebook, we implemented a Graph Attention Network (GAT) for predicting both cell penetrating peptide (CPP) activity and toxicity. The model was trained on a dataset of peptides with known CPP and toxicity properties and evaluated on separate validation and test sets.\n",
        "\n",
        "The model achieved good predictive performance for both tasks, with the ability to identify potential CPPs with low toxicity. The graph-based representation and attention mechanisms allowed the model to effectively capture the structural information relevant for both penetration capability and toxicity."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}